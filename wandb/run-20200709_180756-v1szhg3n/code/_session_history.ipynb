{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import string\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pickle import load, dump\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications import ResNet152V2\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "# from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "from keras import Input\n",
    "from keras.layers import Dropout, Dense, Embedding, LSTM\n",
    "from keras.layers.merge import add\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/vipul43/model5/runs/v1szhg3n"
     ]
    }
   ],
   "source": [
    "#RUN THIS CELL ONLY ONCE\n",
    "import wandb\n",
    "wandb.init(project=\"model5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACTING TEXT DATA AND PREPROCESSING\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr8k.token.txt\"\n",
    "file = open(filepath, 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A DICTIONARY FOR TIME EFFICIENCY PURPOSES\n",
    "tokens_dic = {}\n",
    "for line in lines:\n",
    "    tokens = line.split()\n",
    "    image_ = tokens[0]\n",
    "    caption = ' '.join(tokens[1:])\n",
    "    image_title = image_.split('.')[0]\n",
    "    if(image_title not in tokens_dic.keys()):\n",
    "        tokens_dic[image_title] = [caption]\n",
    "    else:\n",
    "        tokens_dic[image_title].append(caption)\n",
    "\n",
    "\n",
    "#STEP: PREPROCESSING\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for captions in tokens_dic.values():\n",
    "    for i in range(len(captions)):\n",
    "        caption = captions[i]\n",
    "        words = caption.split()\n",
    "        words = [word.lower() for word in words]\n",
    "        words = [word.translate(table) for word in words]\n",
    "        words = [word for word in words if len(word)>1]\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        captions[i] = ' '.join(words)\n",
    "        \n",
    "\n",
    "#LOADING TRAINING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "train = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    train.append(image_title)\n",
    "#NOTE: SIZE OF TRAIN LIST IS 6000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "train_dataset = {}\n",
    "max_caption_length = 0\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in train and (image_title not in train_dataset.keys())):\n",
    "        train_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            train_dataset[image_title].append(refined_caption)\n",
    "            max_caption_length = max(max_caption_length, len(refined_caption.split()))\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000\n",
    "\n",
    "\n",
    "#LOADING CROSS VALIDATION(OR DEVELOPMENT) SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.devImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "dev = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    dev.append(image_title)\n",
    "#NOTE: SIZE OF TEST LIST IS 1000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "dev_dataset = {}\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in dev and (image_title not in dev_dataset.keys())):\n",
    "        dev_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            dev_dataset[image_title].append(refined_caption)\n",
    "#NOTE: SIZE OF DEV DATASET IS 1000\n",
    "\n",
    "\n",
    "#LOADING TESTING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "test = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    test.append(image_title)\n",
    "#NOTE: SIZE OF TEST LIST IS 1000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "test_dataset = {}\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in test and (image_title not in test_dataset.keys())):\n",
    "        test_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            test_dataset[image_title].append(refined_caption)\n",
    "#NOTE: SIZE OF TEST DATASET IS 1000\n",
    "\n",
    "\n",
    "#CONSTRUCTING VOCABULARY FROM CAPTIONS\n",
    "vocabulary = set()\n",
    "for captions in tokens_dic.values():\n",
    "    for caption in captions:\n",
    "        for word in caption.split():\n",
    "            vocabulary.add(word)\n",
    "#NOTE: SIZE OF VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 8763\n",
    "\n",
    "#CONSTRUCTING MOST PROBABLE VOCABULARY FROM ALL TRAIN WORDS\n",
    "word_count_threshold = 10\n",
    "all_train_words = []\n",
    "for captions in train_dataset.values():\n",
    "    for caption in captions:\n",
    "        words = caption.split()\n",
    "        for word in words:\n",
    "            all_train_words.append(word)\n",
    "\n",
    "counter = Counter(all_train_words)\n",
    "commons = counter.most_common()\n",
    "most_probable_vocabulary = set()\n",
    "for ele in commons:\n",
    "    if(ele[1]>=word_count_threshold):\n",
    "        most_probable_vocabulary.add(ele[0])\n",
    "vocabulary_size = len(most_probable_vocabulary)\n",
    "#NOTE: SIZE OF MOST PROBABLE VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 1651\n",
    "\n",
    "\n",
    "#READING ENCODED TRAIN IMAGES, ENCODED DEV IMAGES AND ENCODED TEST IMAGES FROM PKL FILES\n",
    "encoded_train_images = {}\n",
    "encoded_dev_images = {}\n",
    "encoded_test_images = {}\n",
    "with open(\"encoded_train_images2.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoded_train_images = load(encoded_pickle)\n",
    "    \n",
    "with open(\"encoded_test_images2.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoded_test_images = load(encoded_pickle)\n",
    "    \n",
    "with open(\"encoded_dev_images2.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoded_dev_images = load(encoded_pickle)\n",
    "\n",
    "\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            photo = photos[key]\n",
    "            for desc in desc_list:\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocabulary_size+1)[0]\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0\n",
    "                \n",
    "                \n",
    "#USEFUL DICTIONARIES\n",
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "index = 1\n",
    "for word in most_probable_vocabulary:\n",
    "    index_to_word[index] = word\n",
    "    word_to_index[word] = index\n",
    "    index+=1\n",
    "\n",
    "\n",
    "#WORD EMBEDDINGS, MAPPING EVERY WORD OF OUR MOST PROBABALE VOCABULARY TO 200 DIMENSION VECTOR, FOR THAT WE WILL BE USING GLOVE\n",
    "embeddings = {}\n",
    "file = open(\"../../../Downloads/glove/glove.6B.200d.txt\", \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "for line in lines:\n",
    "    word_and_vector = line.split()\n",
    "    word = word_and_vector[0]\n",
    "    vector = word_and_vector[1:]\n",
    "    vector = np.asarray(vector, dtype='float32')\n",
    "    embeddings[word] = vector\n",
    "    \n",
    "    \n",
    "#MAPPING MOST PROBABLE VOCABULARY TO VECTOR BY CREATING A DENSE MATRIX WITH ROW AS WORD AND COLUMNS ARE 200 DIMENSIONS OF VECTOR\n",
    "embedding_dim = 200\n",
    "embeddings_matrix = np.zeros((vocabulary_size+1, embedding_dim))\n",
    "for word, index in word_to_index.items():\n",
    "    vector = embeddings.get(word)\n",
    "    if(vector is not None):\n",
    "        embeddings_matrix[index]=vector\n",
    "#NOTE: DIMENSION OF EMBEDDINGS MATRIX IS (MOST PROBABLE VOCABULARY X 200), HERE (1951 X 200)\n",
    "#NOTE: 1ST ROW OF EMBEDDINGS MATRIX IS ALL ZEROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME ARCHITECTURE AS MODEL\n",
    "input1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(input1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "input2 = Input(shape=(max_caption_length,))\n",
    "se1 = Embedding(vocabulary_size+1, embedding_dim, mask_zero=True)(input2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\n",
    "output = Dense(vocabulary_size+1, activation='softmax')(decoder2)\n",
    "model5 = Model(inputs=[input1, input2], outputs=output, name=\"model5\")\n",
    "\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTING ADDITIONAL PROPERTIES FOR EMBEDDING LAYER\n",
    "model5.layers[2].set_weights([embeddings_matrix])\n",
    "model5.layers[2].trainable = False\n",
    "\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.dropout = 0.5\n",
    "wandb.config.hidden_layer_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS OF TRAINING SESSION 1\n",
    "#NOTE: DEFAULT LEARNING RATE OF KERAS MODEL IS 0.001\n",
    "epochs = 20\n",
    "batch_size = 3\n",
    "steps = len(train_dataset)//batch_size\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.epochs=20\n",
    "wandb.config.batch_size=3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
