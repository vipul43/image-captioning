{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import string\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pickle import load, dump\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras import Input\n",
    "from keras.layers import Dropout, Dense, Embedding, LSTM\n",
    "from keras.layers.merge import add\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACTING TEXT DATA AND PREPROCESSING\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr8k.token.txt\"\n",
    "file = open(filepath, 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A DICTIONARY FOR TIME EFFICIENCY PURPOSES\n",
    "tokens_dic = {}\n",
    "for line in lines:\n",
    "    tokens = line.split()\n",
    "    image_ = tokens[0]\n",
    "    caption = ' '.join(tokens[1:])\n",
    "    image_title = image_.split('.')[0]\n",
    "    if(image_title not in tokens_dic.keys()):\n",
    "        tokens_dic[image_title] = [caption]\n",
    "    else:\n",
    "        tokens_dic[image_title].append(caption)\n",
    "\n",
    "\n",
    "#STEP: PREPROCESSING\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for captions in tokens_dic.values():\n",
    "    for i in range(len(captions)):\n",
    "        caption = captions[i]\n",
    "        words = caption.split()\n",
    "        words = [word.lower() for word in words]\n",
    "        words = [word.translate(table) for word in words]\n",
    "        words = [word for word in words if len(word)>1]\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        captions[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING TRAINING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "train = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    train.append(image_title)\n",
    "#NOTE: SIZE OF TRAIN LIST IS 6000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "train_dataset = {}\n",
    "max_caption_length = 0\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in train and (image_title not in train_dataset.keys())):\n",
    "        train_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            train_dataset[image_title].append(refined_caption)\n",
    "            max_caption_length = max(max_caption_length, len(refined_caption.split()))\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING TESTING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "test = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    test.append(image_title)\n",
    "#NOTE: SIZE OF TEST LIST IS 1000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "test_dataset = {}\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in test and (image_title not in test_dataset.keys())):\n",
    "        test_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            test_dataset[image_title].append(refined_caption)\n",
    "#NOTE: SIZE OF TEST DATASET IS 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTRUCTING VOCABULARY FROM CAPTIONS\n",
    "vocabulary = set()\n",
    "for captions in tokens_dic.values():\n",
    "    for caption in captions:\n",
    "        for word in caption.split():\n",
    "            vocabulary.add(word)\n",
    "#NOTE: SIZE OF VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 8763\n",
    "\n",
    "#CONSTRUCTING MOST PROBABLE VOCABULARY FROM ALL TRAIN WORDS\n",
    "word_count_threshold = 10\n",
    "all_train_words = []\n",
    "for captions in train_dataset.values():\n",
    "    for caption in captions:\n",
    "        words = caption.split()\n",
    "        for word in words:\n",
    "            all_train_words.append(word)\n",
    "\n",
    "counter = Counter(all_train_words)\n",
    "commons = counter.most_common()\n",
    "most_probable_vocabulary = set()\n",
    "for ele in commons:\n",
    "    if(ele[1]>=word_count_threshold):\n",
    "        most_probable_vocabulary.add(ele[0])\n",
    "vocabulary_size = len(most_probable_vocabulary)\n",
    "#NOTE: SIZE OF MOST PROBABLE VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 1651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th ITERATION\n",
      "TIME SO FAR: 4.357329s\n",
      "500th ITERATION\n",
      "TIME SO FAR: 104.253102s\n",
      "1000th ITERATION\n",
      "TIME SO FAR: 203.841536s\n",
      "1500th ITERATION\n",
      "TIME SO FAR: 327.538582s\n",
      "2000th ITERATION\n",
      "TIME SO FAR: 421.301079s\n",
      "2500th ITERATION\n",
      "TIME SO FAR: 513.465127s\n",
      "3000th ITERATION\n",
      "TIME SO FAR: 605.739588s\n",
      "3500th ITERATION\n",
      "TIME SO FAR: 698.434162s\n",
      "4000th ITERATION\n",
      "TIME SO FAR: 791.074074s\n",
      "4500th ITERATION\n",
      "TIME SO FAR: 885.560459s\n",
      "5000th ITERATION\n",
      "TIME SO FAR: 978.235567s\n",
      "5500th ITERATION\n",
      "TIME SO FAR: 1071.538087s\n",
      "TIME TAKEN TO ENCODE TRAIN IMAGES INTO 2048 LENGTH FEATURE VECTOR: 1164.811601s\n"
     ]
    }
   ],
   "source": [
    "#TO GET INPUT VECTOR X FROM IMAGES WE USE TRANSFER LEARNING THROUGH INCEPTIONV3 MODEL TRAINED ON 1000 DIFFERENT CLASSES OF IMAGES\n",
    "model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(model.input, model.layers[-2].output)\n",
    "encoded_train_images = {}\n",
    "start = dt.datetime.now()\n",
    "i = 0\n",
    "for image_title in train_dataset.keys():\n",
    "    image_path = \"../../../Downloads/Flickr8k/Flicker8k_dataset/\"+image_title+\".jpg\"\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feature_vec = model_new.predict(x)\n",
    "    feature_vec = np.reshape(feature_vec, feature_vec.shape[1])\n",
    "    encoded_train_images[image_title] = feature_vec\n",
    "    present = dt.datetime.now()\n",
    "    if((i/500).is_integer()):\n",
    "        print(\"{}th ITERATION\".format(i))\n",
    "        print(\"TIME SO FAR: {}s\".format((present-start).total_seconds()))\n",
    "    i+=1\n",
    "end = dt.datetime.now()\n",
    "print(\"TIME TAKEN TO ENCODE TRAIN IMAGES INTO 2048 LENGTH FEATURE VECTOR: {}s\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th ITERATION\n",
      "TIME SO FAR: 3.734712s\n",
      "500th ITERATION\n",
      "TIME SO FAR: 96.06271s\n",
      "TIME TAKEN TO ENCODE TEST IMAGES INTO 2048 LENGTH FEATURE VECTOR: 189.069741s\n"
     ]
    }
   ],
   "source": [
    "#TO GET INPUT VECTOR X FROM IMAGES WE USE TRANSFER LEARNING THROUGH INCEPTIONV3 MODEL TRAINED ON 1000 DIFFERENT CLASSES OF IMAGES\n",
    "model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(model.input, model.layers[-2].output)\n",
    "encoded_test_images = {}\n",
    "start = dt.datetime.now()\n",
    "i = 0\n",
    "for image_title in test_dataset.keys():\n",
    "    image_path = \"../../../Downloads/Flickr8k/Flicker8k_dataset/\"+image_title+\".jpg\"\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feature_vec = model_new.predict(x)\n",
    "    feature_vec = np.reshape(feature_vec, feature_vec.shape[1])\n",
    "    encoded_test_images[image_title] = feature_vec\n",
    "    present = dt.datetime.now()\n",
    "    if((i/500).is_integer()):\n",
    "        print(\"{}th ITERATION\".format(i))\n",
    "        print(\"TIME SO FAR: {}s\".format((present-start).total_seconds()))\n",
    "    i+=1\n",
    "end = dt.datetime.now()\n",
    "print(\"TIME TAKEN TO ENCODE TEST IMAGES INTO 2048 LENGTH FEATURE VECTOR: {}s\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITING ENCODED TRAIN IMAGES AND ENCODED TEST IMAGES TO PKL FILES\n",
    "with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoded_train_images, encoded_pickle)\n",
    "    \n",
    "with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoded_test_images, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key]\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocabulary_size+1)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USEFUL DICTIONARIES\n",
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "index = 1\n",
    "for word in most_probable_vocabulary:\n",
    "    index_to_word[index] = word\n",
    "    word_to_index[word] = index\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD EMBEDDINGS, MAPPING EVERY WORD OF OUR MOST PROBABALE VOCABULARY TO 200 DIMENSION VECTOR, FOR THAT WE WILL BE USING GLOVE\n",
    "embeddings = {}\n",
    "file = open(\"../../../Downloads/glove/glove.6B.200d.txt\", \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "for line in lines:\n",
    "    word_and_vector = line.split()\n",
    "    word = word_and_vector[0]\n",
    "    vector = word_and_vector[1:]\n",
    "    vector = np.asarray(vector, dtype='float32')\n",
    "    embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING MOST PROBABLE VOCABULARY TO VECTOR BY CREATING A DENSE MATRIX WITH ROW AS WORD AND COLUMNS ARE 200 DIMENSIONS OF VECTOR\n",
    "embedding_dim = 200\n",
    "embeddings_matrix = np.zeros((vocabulary_size+1, embedding_dim))\n",
    "for word, index in word_to_index.items():\n",
    "    vector = embeddings.get(word)\n",
    "    if(vector is not None):\n",
    "        embeddings_matrix[index]=vector\n",
    "#NOTE: DIMENSION OF EMBEDDINGS MATRIX IS (MOST PROBABLE VOCABULARY X 200), HERE (1951 X 200)\n",
    "#NOTE: 1ST ROW OF EMBEDDINGS MATRIX IS ALL ZEROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 200)      330400      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1652)         424564      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,813,268\n",
      "Trainable params: 1,813,268\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DEFINING THE LAYERS OF THE MODEL\n",
    "input1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(input1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "input2 = Input(shape=(max_caption_length,))\n",
    "se1 = Embedding(vocabulary_size+1, embedding_dim, mask_zero=True)(input2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\n",
    "output = Dense(vocabulary_size+1, activation='softmax')(decoder2)\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTING ADDITIONAL PROPERTIES FOR PARTICULAR LAYER\n",
    "model.layers[2].set_weights([embeddings_matrix])\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS OF TRAINING SESSION 1\n",
    "#NOTE: DEFAULT LEARNING RATE OF KERAS MODEL IS 0.001\n",
    "epochs = 20\n",
    "batch_size = 3\n",
    "steps = len(train_dataset)//batch_size\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 336/2000 [====>.........................] - ETA: 10:20 - loss: 4.9867"
     ]
    }
   ],
   "source": [
    "#TRAINING SESSION 1\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_dataset, encoded_train_images, word_to_index, max_caption_length, batch_size)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./model_weights2/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS OF TRAINING SESSION 2\n",
    "#NOTE: LEARNING RATE IS CHANGED TO 0.0001\n",
    "K.set_value(model.optimizer.lr, 0.0001)\n",
    "epochs = 10\n",
    "batch_size = 6\n",
    "steps = len(train_dataset)//batch_size\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING SESSION 2\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_dataset, encoded_train_images, word_to_index, max_caption_length, batch_size)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./model_weights2/model_' + str(i+20) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING TRAINED WEIGHTS TO MODEL\n",
    "model.load_weights('./model_weights2/model_29.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(feature_vec):\n",
    "    partial_caption = \"startseq\"\n",
    "    for i in range(max_caption_length):\n",
    "        seq = [word_to_index[word] for word in partial_caption.split() if word in word_to_index]\n",
    "        seq = pad_sequences([seq], maxlen=max_caption_length)\n",
    "        yhat = model.predict([feature_vec,seq], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = index_to_word[yhat]\n",
    "        partial_caption += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final_caption = partial_caption.split()[1:-1]\n",
    "    final_caption = ' '.join(final_caption)\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST DATASET RESULTS VIEWING\n",
    "for image_title in list(encoded_test_images.keys())[:10]:\n",
    "    feature_vec = encoded_test_images[image_title].reshape((1, 2048))\n",
    "    image_path = \"../../../Downloads/Flickr8k/Flicker8k_dataset/\"+image_title+\".jpg\"\n",
    "    image = plt.imread(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(\"prediction: {}\".format(predict(feature_vec)))\n",
    "    print(\"ground truth: {}\".format(' '.join(test_dataset[image_title][0].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(test_dataset[image_title][1].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(test_dataset[image_title][2].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(test_dataset[image_title][3].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(test_dataset[image_title][4].split()[1:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN DATASET RESULTS VIEWING\n",
    "for image_title in list(encoded_train_images.keys())[:10]:\n",
    "    feature_vec = encoded_train_images[image_title].reshape((1, 2048))\n",
    "    image_path = \"../../../Downloads/Flickr8k/Flicker8k_dataset/\"+image_title+\".jpg\"\n",
    "    image = plt.imread(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(\"prediction: {}\".format(predict(feature_vec)))\n",
    "    print(\"ground truth: {}\".format(' '.join(train_dataset[image_title][0].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(train_dataset[image_title][1].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(train_dataset[image_title][2].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(train_dataset[image_title][3].split()[1:-1])))\n",
    "    print(\"ground truth: {}\".format(' '.join(train_dataset[image_title][4].split()[1:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
