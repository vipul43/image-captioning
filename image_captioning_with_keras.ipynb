{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import string\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "#EXTRACTING TEXT DATA AND PREPROCESSING\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr8k.token.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A DICTIONARY FOR TIME EFFICIENCY PURPOSES\n",
    "tokens_dic = {}\n",
    "for line in lines:\n",
    "    [image_, caption] = line.split('\\t')\n",
    "    image_title = image_.split('.')[0]\n",
    "    if(image_title not in tokens_dic.keys()):\n",
    "        tokens_dic[image_title] = [caption]\n",
    "    else:\n",
    "        tokens_dic[image_title].append(caption)\n",
    "\n",
    "#STEP: PREPROCESSING\n",
    "table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "max_caption_length = 0\n",
    "total_words_list = []\n",
    "for captions in tokens_dic.values():\n",
    "    for i in range(len(captions)):\n",
    "        caption = captions[i]\n",
    "        words = caption.split()\n",
    "        words = [word.lower() for word in words]\n",
    "        words = [word.translate(table) for word in words]\n",
    "#         words = [word for word in words if len(word)>1]\n",
    "        words = [word+' ' for word in words if word.isalpha()]\n",
    "        total_words_list.extend(words)\n",
    "        captions[i] = ''.join(words)\n",
    "        max_caption_length = max(max_caption_length, len(words))\n",
    "#NOTE: MAX CAPTION LENGTH BY LETTING 1 LENGTH WORDS BE IN CAPTION IS 35\n",
    "#NOTE: MAX CAPTION LENGTH BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 32\n",
    "print(max_caption_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTRUCTING VOCABULARY FROM CAPTIONS\n",
    "vocabulary = set()\n",
    "for captions in tokens_dic.values():\n",
    "    for caption in captions:\n",
    "        for word in caption.split():\n",
    "            vocabulary.add(word)\n",
    "#NOTE: SIZE OF VOCABULARY BY LETTING 1 LENGTH WORDS BE IN CAPTION IS 8775\n",
    "#NOTE: SIZE OF VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 8763\n",
    "\n",
    "#CONSTRUCTING MOST PROBABLE VOCABULARY FROM ALL WORDS\n",
    "counter = Counter(total_words_list)\n",
    "commons = counter.most_common()\n",
    "most_probable_vocabulary = set()\n",
    "for tup in commons:\n",
    "    if(tup[1]>=10):\n",
    "        most_probable_vocabulary.add(tup[0])\n",
    "#NOTE: SIZE OF MOST PROBABLE VOCABULARY BY LETTING 1 LENGTH WORDS BE IN CAPTION IS 1950\n",
    "#NOTE: SIZE OF MOST PROBABLE VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 1947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING TRAINING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "train = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    train.append(image_title)\n",
    "#NOTE: SIZE OF TRAIN LIST IS 6000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "train_dataset = {}\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in train and image_title not in train_dataset.keys()):\n",
    "        train_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \"endseq\"\n",
    "            train_dataset[image_title].append(refined_caption)\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING TESTING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "test = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    test.append(image_title)\n",
    "#NOTE: SIZE OF TEST LIST IS 1000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "test_dataset = {}\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in test and image_title not in test_dataset.keys()):\n",
    "        test_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \"endseq\"\n",
    "            test_dataset[image_title].append(refined_caption)\n",
    "#NOTE: SIZE OF TEST DATASET IS 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th ITERATION\n",
      "TIME SO FAR: 8.841215s\n",
      "500th ITERATION\n",
      "TIME SO FAR: 218.930252s\n",
      "1000th ITERATION\n",
      "TIME SO FAR: 359.79328s\n",
      "1500th ITERATION\n",
      "TIME SO FAR: 455.496382s\n",
      "2000th ITERATION\n",
      "TIME SO FAR: 549.198238s\n",
      "2500th ITERATION\n",
      "TIME SO FAR: 644.261416s\n",
      "3000th ITERATION\n",
      "TIME SO FAR: 744.530409s\n",
      "3500th ITERATION\n",
      "TIME SO FAR: 848.340382s\n",
      "4000th ITERATION\n",
      "TIME SO FAR: 954.615744s\n",
      "4500th ITERATION\n",
      "TIME SO FAR: 1063.575684s\n",
      "5000th ITERATION\n",
      "TIME SO FAR: 1175.704234s\n",
      "5500th ITERATION\n",
      "TIME SO FAR: 1292.065183s\n",
      "TIME TAKEN TO ENCODE TRAIN IMAGES INTO 2048 LENGTH FEATURE VECTOR: 1406.20231s\n"
     ]
    }
   ],
   "source": [
    "#TO GET INPUT VECTOR X FROM IMAGES WE USE TRANSFER LEARNING THROUGH INCEPTIONV3 MODEL TRAINED ON 1000 DIFFERENT CLASSES OF IMAGES\n",
    "model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(model.input, model.layers[-2].output)\n",
    "encoded_train_images = {}\n",
    "start = dt.datetime.now()\n",
    "i = 0\n",
    "for image_title in train_dataset.keys():\n",
    "    image_path = \"../../../Downloads/Flickr8k/Flicker8k_dataset/\"+image_title+\".jpg\"\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feature_vec = model_new.predict(x)\n",
    "    feature_vec = np.reshape(feature_vec, feature_vec.shape[1])\n",
    "    encoded_train_images[image_title] = feature_vec\n",
    "    present = dt.datetime.now()\n",
    "    if((i/500).is_integer()):\n",
    "        print(\"{}th ITERATION\".format(i))\n",
    "        print(\"TIME SO FAR: {}s\".format((present-start).total_seconds()))\n",
    "    i+=1\n",
    "end = dt.datetime.now()\n",
    "print(\"TIME TAKEN TO ENCODE TRAIN IMAGES INTO 2048 LENGTH FEATURE VECTOR: {}s\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th ITERATION\n",
      "TIME SO FAR: 5.82063s\n",
      "500th ITERATION\n",
      "TIME SO FAR: 120.00942s\n",
      "TIME TAKEN TO ENCODE TEST IMAGES INTO 2048 LENGTH FEATURE VECTOR: 234.424786s\n"
     ]
    }
   ],
   "source": [
    "#TO GET INPUT VECTOR X FROM IMAGES WE USE TRANSFER LEARNING THROUGH INCEPTIONV3 MODEL TRAINED ON 1000 DIFFERENT CLASSES OF IMAGES\n",
    "model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(model.input, model.layers[-2].output)\n",
    "encoded_test_images = {}\n",
    "start = dt.datetime.now()\n",
    "i = 0\n",
    "for image_title in test_dataset.keys():\n",
    "    image_path = \"../../../Downloads/Flickr8k/Flicker8k_dataset/\"+image_title+\".jpg\"\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feature_vec = model_new.predict(x)\n",
    "    feature_vec = np.reshape(feature_vec, feature_vec.shape[1])\n",
    "    encoded_test_images[image_title] = feature_vec\n",
    "    present = dt.datetime.now()\n",
    "    if((i/500).is_integer()):\n",
    "        print(\"{}th ITERATION\".format(i))\n",
    "        print(\"TIME SO FAR: {}s\".format((present-start).total_seconds()))\n",
    "    i+=1\n",
    "end = dt.datetime.now()\n",
    "print(\"TIME TAKEN TO ENCODE TEST IMAGES INTO 2048 LENGTH FEATURE VECTOR: {}s\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITING ENCODED TRAIN IMAGES AND ENCODED TEST IMAGES TO PKL FILES\n",
    "\n",
    "with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoded_train_images, encoded_pickle)\n",
    "    \n",
    "with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoded_test_images, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
