{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "import string\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pickle import load, dump\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications import ResNet152V2\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "# from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "from keras import Input\n",
    "from keras.layers import Dropout, Dense, Embedding, LSTM\n",
    "from keras.layers.merge import add\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/vipul43/model4\" target=\"_blank\">https://app.wandb.ai/vipul43/model4</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/vipul43/model4/runs/15erpkah\" target=\"_blank\">https://app.wandb.ai/vipul43/model4/runs/15erpkah</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/vipul43/model4/runs/15erpkah"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL ONLY ONCE\n",
    "import wandb\n",
    "wandb.init(project=\"model4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACTING TEXT DATA AND PREPROCESSING\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr8k.token.txt\"\n",
    "file = open(filepath, 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A DICTIONARY FOR TIME EFFICIENCY PURPOSES\n",
    "tokens_dic = {}\n",
    "for line in lines:\n",
    "    tokens = line.split()\n",
    "    image_ = tokens[0]\n",
    "    caption = ' '.join(tokens[1:])\n",
    "    image_title = image_.split('.')[0]\n",
    "    if(image_title not in tokens_dic.keys()):\n",
    "        tokens_dic[image_title] = [caption]\n",
    "    else:\n",
    "        tokens_dic[image_title].append(caption)\n",
    "\n",
    "\n",
    "#STEP: PREPROCESSING\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for captions in tokens_dic.values():\n",
    "    for i in range(len(captions)):\n",
    "        caption = captions[i]\n",
    "        words = caption.split()\n",
    "        words = [word.lower() for word in words]\n",
    "        words = [word.translate(table) for word in words]\n",
    "        words = [word for word in words if len(word)>1]\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        captions[i] = ' '.join(words)\n",
    "        \n",
    "\n",
    "#LOADING TRAINING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "train = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    train.append(image_title)\n",
    "#NOTE: SIZE OF TRAIN LIST IS 6000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "train_dataset = {}\n",
    "max_caption_length = 0\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in train and (image_title not in train_dataset.keys())):\n",
    "        train_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            train_dataset[image_title].append(refined_caption)\n",
    "            max_caption_length = max(max_caption_length, len(refined_caption.split()))\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000\n",
    "\n",
    "\n",
    "#LOADING CROSS VALIDATION(OR DEVELOPMENT) SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.devImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "dev = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    dev.append(image_title)\n",
    "#NOTE: SIZE OF TEST LIST IS 1000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "dev_dataset = {}\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in dev and (image_title not in dev_dataset.keys())):\n",
    "        dev_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            dev_dataset[image_title].append(refined_caption)\n",
    "#NOTE: SIZE OF DEV DATASET IS 1000\n",
    "\n",
    "\n",
    "#LOADING TESTING SET\n",
    "\n",
    "#STEP: EXTRACTING DATA\n",
    "filepath = \"../../../Downloads/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt\"\n",
    "file = open(filepath, \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "\n",
    "#STEP: SAVING DATA IN A LIST\n",
    "test = []\n",
    "for line in lines:\n",
    "    [image_title, stuff] =  line.split('.')\n",
    "    test.append(image_title)\n",
    "#NOTE: SIZE OF TEST LIST IS 1000\n",
    "\n",
    "#STEP: SAVING IMAGE-CAPTIONS IN TRAIN DATASET\n",
    "test_dataset = {}\n",
    "for image_title, captions in tokens_dic.items():\n",
    "    if(image_title in test and (image_title not in test_dataset.keys())):\n",
    "        test_dataset[image_title] = list()\n",
    "        for caption in captions:\n",
    "            refined_caption = \"startseq \" + caption + \" endseq\"\n",
    "            test_dataset[image_title].append(refined_caption)\n",
    "#NOTE: SIZE OF TEST DATASET IS 1000\n",
    "\n",
    "\n",
    "#CONSTRUCTING VOCABULARY FROM CAPTIONS\n",
    "vocabulary = set()\n",
    "for captions in tokens_dic.values():\n",
    "    for caption in captions:\n",
    "        for word in caption.split():\n",
    "            vocabulary.add(word)\n",
    "#NOTE: SIZE OF VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 8763\n",
    "\n",
    "#CONSTRUCTING MOST PROBABLE VOCABULARY FROM ALL TRAIN WORDS\n",
    "word_count_threshold = 10\n",
    "all_train_words = []\n",
    "for captions in train_dataset.values():\n",
    "    for caption in captions:\n",
    "        words = caption.split()\n",
    "        for word in words:\n",
    "            all_train_words.append(word)\n",
    "\n",
    "counter = Counter(all_train_words)\n",
    "commons = counter.most_common()\n",
    "most_probable_vocabulary = set()\n",
    "for ele in commons:\n",
    "    if(ele[1]>=word_count_threshold):\n",
    "        most_probable_vocabulary.add(ele[0])\n",
    "vocabulary_size = len(most_probable_vocabulary)\n",
    "#NOTE: SIZE OF MOST PROBABLE VOCABULARY BY NOT LETTING 1 LENGTH WORDS BE IN CAPTION IS 1651\n",
    "\n",
    "\n",
    "#READING ENCODED TRAIN IMAGES, ENCODED DEV IMAGES AND ENCODED TEST IMAGES FROM PKL FILES\n",
    "encoded_train_images = {}\n",
    "encoded_dev_images = {}\n",
    "encoded_test_images = {}\n",
    "with open(\"encoded_train_images2.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoded_train_images = load(encoded_pickle)\n",
    "    \n",
    "with open(\"encoded_test_images2.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoded_test_images = load(encoded_pickle)\n",
    "    \n",
    "with open(\"encoded_dev_images2.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoded_dev_images = load(encoded_pickle)\n",
    "\n",
    "\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            photo = photos[key]\n",
    "            for desc in desc_list:\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocabulary_size+1)[0]\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0\n",
    "                \n",
    "                \n",
    "#USEFUL DICTIONARIES\n",
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "index = 1\n",
    "for word in most_probable_vocabulary:\n",
    "    index_to_word[index] = word\n",
    "    word_to_index[word] = index\n",
    "    index+=1\n",
    "\n",
    "\n",
    "#WORD EMBEDDINGS, MAPPING EVERY WORD OF OUR MOST PROBABALE VOCABULARY TO 200 DIMENSION VECTOR, FOR THAT WE WILL BE USING GLOVE\n",
    "embeddings = {}\n",
    "file = open(\"../../../Downloads/glove/glove.6B.200d.txt\", \"r\")\n",
    "content = file.read()\n",
    "file.close()\n",
    "lines = content.split('\\n')\n",
    "for line in lines:\n",
    "    word_and_vector = line.split()\n",
    "    word = word_and_vector[0]\n",
    "    vector = word_and_vector[1:]\n",
    "    vector = np.asarray(vector, dtype='float32')\n",
    "    embeddings[word] = vector\n",
    "    \n",
    "    \n",
    "#MAPPING MOST PROBABLE VOCABULARY TO VECTOR BY CREATING A DENSE MATRIX WITH ROW AS WORD AND COLUMNS ARE 200 DIMENSIONS OF VECTOR\n",
    "embedding_dim = 200\n",
    "embeddings_matrix = np.zeros((vocabulary_size+1, embedding_dim))\n",
    "for word, index in word_to_index.items():\n",
    "    vector = embeddings.get(word)\n",
    "    if(vector is not None):\n",
    "        embeddings_matrix[index]=vector\n",
    "#NOTE: DIMENSION OF EMBEDDINGS MATRIX IS (MOST PROBABLE VOCABULARY X 200), HERE (1951 X 200)\n",
    "#NOTE: 1ST ROW OF EMBEDDINGS MATRIX IS ALL ZEROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "<img src=\"files/images/model1_arch.png?1\">\n",
    "<img src=\"files/images/model_summary.png?1\">\n",
    "<img src=\"files/images/model_performance.png?1\">\n",
    "\n",
    "# model2\n",
    "<img src=\"files/images/model2_arch.png?1\">\n",
    "<img src=\"files/images/model2_summary.png?1\">\n",
    "<img src=\"files/images/model2_performance.png?1\">\n",
    "\n",
    "# model3\n",
    "<img src=\"files/images/model3_arch.png?3\">\n",
    "<img src=\"files/images/model3_summary.png?1\">\n",
    "<img src=\"files/images/model3_performance.png?1\">\n",
    "\n",
    "# model4\n",
    "<img src=\"files/images/model4_arch.png?1\">\n",
    "<img src=\"files/images/model4_summary.png?1\">\n",
    "<img src=\"files/images/model4_performance.png?1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 34, 200)      330400      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 34, 200)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          524544      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          467968      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256)          0           dense_4[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1652)         424564      dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,813,268\n",
      "Trainable params: 1,813,268\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#SAME ARCHITECTURE AS MODEL\n",
    "input1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(input1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "input2 = Input(shape=(max_caption_length,))\n",
    "se1 = Embedding(vocabulary_size+1, embedding_dim, mask_zero=True)(input2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\n",
    "output = Dense(vocabulary_size+1, activation='softmax')(decoder2)\n",
    "model5 = Model(inputs=[input1, input2], outputs=output, name=\"model5\")\n",
    "\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTING ADDITIONAL PROPERTIES FOR EMBEDDING LAYER\n",
    "model5.layers[2].set_weights([embeddings_matrix])\n",
    "model5.layers[2].trainable = False\n",
    "\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.dropout = 0.5\n",
    "wandb.config.hidden_layer_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS OF TRAINING SESSION 1\n",
    "#NOTE: DEFAULT LEARNING RATE OF KERAS MODEL IS 0.001\n",
    "epochs = 20\n",
    "batch_size = 3\n",
    "steps = len(train_dataset)//batch_size\n",
    "#NOTE: SIZE OF TRAIN DATASET IS 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.epochs=20\n",
    "wandb.config.batch_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 744s 372ms/step - loss: 4.1660 - accuracy: 0.2356\n",
      "Epoch 1/1\n",
      " 365/2000 [====>.........................] - ETA: 11:20 - loss: 3.5889 - accuracy: 0.2758"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-afcb006441ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_train_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_caption_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING SESSION 1\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_dataset, encoded_train_images, word_to_index, max_caption_length, batch_size)\n",
    "    hist = model5.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    loss = hist.history['loss']\n",
    "    wandb.log({'epoch': i, 'loss': loss})\n",
    "model5.save('./model5_weights/model_' + str(0) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
